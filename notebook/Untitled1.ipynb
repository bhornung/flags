{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Extracts words from a corpus and sanitises them.\n",
    "    \"\"\"\n",
    "    \n",
    "    allowed_chars = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    \n",
    "    splitted = corpus.split()\n",
    "    \n",
    "    cleaned = (\"\".join(filter(lambda x: x in allowed_chars, word)) for word in splitted)\n",
    "    cleaned = [x.lower() for x in cleaned]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "class StaticEncoder:\n",
    "\n",
    "    def __init__(self, corpus, \n",
    "                 default_missing_key = -1,\n",
    "                 default_missing_val = \"\"):\n",
    "\n",
    "        self._decoder = dict(enumerate(set(corpus)))\n",
    "\n",
    "        self._encoder = {v : k for k,v in self._decoder.items()}\n",
    "        \n",
    "        self._default_missing_key = default_missing_key\n",
    "        \n",
    "        self._default_missing_val = default_missing_val\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        \n",
    "        return self._encoder.get(x, self._default_missing_key)\n",
    "    \n",
    "    \n",
    "    def decode(self, x):\n",
    "        \n",
    "        return self._decoder.get(x, self._default_missing_val)\n",
    "    \n",
    "train_corpus = \"\"\"\n",
    "Mary had a little lamb its fleece was white as snow;\n",
    "And everywhere that Mary went, the lamb was sure to go. \n",
    "It followed her to school one day, which was against the rule;\n",
    "It made the children laugh and play, to see a lamb at school.\n",
    "And so the teacher turned it out, but still it lingered near,\n",
    "And waited patiently about till Mary did appear.\n",
    "\"Why does the lamb love Mary so?\" the eager children cry;\n",
    "\"Why, Mary loves the lamb, you know\" the teacher did reply.\"\n",
    "\"\"\"\n",
    "\n",
    "cleaned_corpus = _clean_corpus(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_input_string(string):\n",
    "    \n",
    "    components = string.split(',')\n",
    "    \n",
    "    try:\n",
    "        n = int(components[0])\n",
    "    except:\n",
    "        raise ValueError(\"Cannot covert to int: malformatted string.\")\n",
    "        \n",
    "    words = []\n",
    "    for x in components[1:]:\n",
    "        words.extend(x.split())\n",
    "        \n",
    "    words = tuple([x.lower() for x in words])\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        raise ValueError(\"No words in input string\")\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_encoder = StaticEncoder(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def _generate_all_grams(corpus, max_len = 4):\n",
    "    \"\"\"\n",
    "    Creates a generator of all n_grams between 2, and len(corpus) -1\n",
    "    Parameters:\n",
    "        corpus ([str]) : corpus\n",
    "    \"\"\"\n",
    "    max_len_ = min(len(corpus), max_len+1)\n",
    "    n_gram_generator = chain(*(zip(*(corpus[j:] for j in range(i))) for i in range(2, max_len_)))\n",
    "    \n",
    "    return n_gram_generator\n",
    "\n",
    "\n",
    "def _create_n_gram_counter(n_gram_generator):\n",
    "    \"\"\"\n",
    "    Calculates the unnormalised transition matrix as a counter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # split n_grams to ((n-1), 1) pairs\n",
    "    split_n_grams = ((x[:-1], x[-1]) for x in n_gram_generator)\n",
    "    \n",
    "    # count pairs\n",
    "    n_gram_counter = Counter(split_n_grams)\n",
    "\n",
    "    return n_gram_counter\n",
    "\n",
    "\n",
    "def _create_probability_dict(n_gram_counter):\n",
    "    \"\"\"\n",
    "    Calculates the transition probabilities as a dict of dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    prob_dict = {}\n",
    "\n",
    "    for (head, tail), count in n_gram_counter.items():\n",
    "    \n",
    "        if not head in prob_dict:\n",
    "            prob_dict.update({head : {tail : count}})\n",
    "        else:\n",
    "            prob_dict[head].update({tail : count})\n",
    "\n",
    "    ## normalise to unit probabilities\n",
    "    for head, tail_counts in prob_dict.items():\n",
    "        \n",
    "        tail_counts = {tail : count * 1.0 / sum(tail_counts.values()) \n",
    "                           for tail, count in tail_counts.items()}\n",
    "        \n",
    "        prob_dict[head] = tail_counts\n",
    "        \n",
    "    return prob_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramPredictor:\n",
    "    \n",
    "    \n",
    "    def __init__(self, prob_dict, encoder):\n",
    "        \"\"\"\n",
    "        Creates an instance of the n-gram predictor with a trained\n",
    "        transition matrix.\n",
    "        Parameters\n",
    "            prob_dict ({str : {str : float}}) : transition probabilites\n",
    "        \"\"\"\n",
    "        \n",
    "        self._encoder = encoder\n",
    "        self._prob_dict = prob_dict\n",
    "        \n",
    "    \n",
    "    def predict(self, pattern):\n",
    "        \"\"\"\n",
    "        Lists the predictions of the subsequent word based on an input string.\n",
    "        \"\"\"\n",
    "        \n",
    "        pattern_ = tuple((self._encoder.encode(x) for x in pattern))\n",
    "        \n",
    "        probabilites = self._find_probability(pattern_)\n",
    "        \n",
    "        self._print_probabilites(probabilites)\n",
    "        \n",
    "\n",
    "    def _print_probabilites(self, probabilites):\n",
    "        \"\"\"\n",
    "        Prints the search results.\n",
    "        Parameters:\n",
    "            search_result ({str:float} or None) : probabilities of the following word.\n",
    "        \"\"\"\n",
    "        \n",
    "        if probabilites is None:\n",
    "            print(\"No prediction available\")\n",
    "            \n",
    "        else:\n",
    "            sorted_ = [v for v in sorted(probabilites.items(), key = lambda x: (-x[1], x[0]))]\n",
    "            sorted_ = [(self._encoder.decode(k), v) for k, v in sorted_]\n",
    "            string_components = [\"{0},{1:.3f}\".format(word, prob) for word, prob in sorted_]\n",
    "            string = \";\".join(string_components)\n",
    "            print(string)\n",
    "    \n",
    "        \n",
    "    def _find_probability(self, pattern):\n",
    "        \"\"\"\n",
    "        Tries to find the transition probabilites.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self._prob_dict.get(pattern, None)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [corpus_encoder.encode(x) for x in cleaned_corpus]\n",
    "\n",
    "n_gram_generator = _generate_all_grams(encoded, max_len = 4)\n",
    "n_gram_counter = _create_n_gram_counter(n_gram_generator)\n",
    "prob_dict = _create_probability_dict(n_gram_counter)\n",
    "\n",
    "n_gram_predictor = NGramPredictor(prob_dict, corpus_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb,0.375;teacher,0.250;children,0.125;eager,0.125;rule,0.125\n"
     ]
    }
   ],
   "source": [
    "pattern = _prepare_input_string(\"2,the\")\n",
    "n_gram_predictor.predict(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print(line, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b78f81938655>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'b'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'c'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m0.59\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "d = {'b' : 0.25, 'a' : 0.25, 'c' : 0.59}\n",
    "\n",
    "sorted(k.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "[v for v in sorted(d.items(), key = lambda kv: (-kv[1], kv[0]))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
